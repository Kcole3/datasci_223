{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ID   Cohort  Year of ICI treatment start Age at ICI start     Sex  \\\n",
      "0  8213  Initial                         2017          61 - 65  Female   \n",
      "1  8214  Initial                         2018          66 - 70  Female   \n",
      "2  8215  Initial                         2018          61 - 65  Female   \n",
      "3  8216  Initial                         2015          56 - 60  Female   \n",
      "4  8217  Initial                         2018          51 - 55  Female   \n",
      "\n",
      "   Cancer type   NLR  MSI type  TMB (Mutations/Mb) ICI drug class  \\\n",
      "0   Colorectal  7.60       NaN                 4.9     PD-1/PD-L1   \n",
      "1  Endometrial  1.19  Unstable                32.5     PD-1/PD-L1   \n",
      "2         SCLC  1.38    Stable                19.3     PD-1/PD-L1   \n",
      "3     Melanoma  2.69    Stable                 1.0          Combo   \n",
      "4        NSCLC  2.54    Stable                10.5     PD-1/PD-L1   \n",
      "\n",
      "  Best Response Response to ICI Clinical Benefit with ICI Progression  \\\n",
      "0            PD              No                        No         Yes   \n",
      "1            PR             Yes                       Yes          No   \n",
      "2            PR             Yes                       Yes         Yes   \n",
      "3            PD              No                        No         Yes   \n",
      "4            PR             Yes                       Yes         Yes   \n",
      "\n",
      "   Progression-Free Survival (Months) Vital status  Overall Survival (Months)  \\\n",
      "0                                0.92         Dead                       5.62   \n",
      "1                               15.64        Alive                      15.74   \n",
      "2                                3.45        Alive                       9.59   \n",
      "3                                0.56        Alive                      50.14   \n",
      "4                                4.67        Alive                       9.99   \n",
      "\n",
      "  Stage at ICI start ICI line of treatment  ECOG PS  \n",
      "0                 IV       Subsequent-line      0.0  \n",
      "1                 IV       Subsequent-line      1.0  \n",
      "2                 IV       Subsequent-line      0.0  \n",
      "3                 IV            First-line      0.0  \n",
      "4                 IV       Subsequent-line      0.0  \n"
     ]
    }
   ],
   "source": [
    "#Import the data\n",
    "import pandas as pd\n",
    "#%pip install openpyxl\n",
    "\n",
    "excel_file_path = '/Users/katherinecole/Downloads/Oncology_IO Data.xlsx'\n",
    "\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick Descriptive statistics- \n",
    "\n",
    "#Get list of column name. These names will be modified later in the code to simplify analysis\n",
    "print(df.columns. values. tolist())\n",
    "\n",
    "#Identify number of rows and columns\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify column types\n",
    "column_types = df.dtypes\n",
    "print(column_types)\n",
    "\n",
    "#Assess for missing values, and make sure missing values are identified correctly\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() #This is another way to view the number of non-missing values, and to get the Dtype for the column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function was adapted from a function that I previously made in R\n",
    "\n",
    "#It essentially modifies all column names to lowercase, replaces any non alphanumeric characters with underscores, and gets rid of XS spaces\n",
    "\n",
    "import re\n",
    "\n",
    "#define the clean_names function\n",
    "def clean_names(x):\n",
    "    #replace all variable names to lowercase\n",
    "    x = x.lower()\n",
    "    # Replace any character NOT alphanumeric with underscore\n",
    "    x = re.sub(r'[^a-zA-Z0-9]', '_', x)\n",
    "    # Replace two or more underscores with a single underscore\n",
    "    x = re.sub(r'_{2,}', '_', x)\n",
    "    # Replace any non-alphabetic character at the beginning with empty string\n",
    "    x = re.sub(r'^[^a-zA-Z]+', '', x)\n",
    "    # Replace any non-alphanumeric character at the end with empty string\n",
    "    x = re.sub(r'[^a-zA-Z0-9]+$', '', x)\n",
    "    return x\n",
    "\n",
    "# Function to clean column names\n",
    "def clean_column_names(df):\n",
    "    df.columns = [clean_names(col) for col in df.columns]\n",
    "    return df\n",
    "\n",
    "# Clean column names\n",
    "data_clean = clean_column_names(df)\n",
    "\n",
    "print(data_clean.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the column names\n",
    "\n",
    "data_clean = data_clean.rename(columns={\n",
    "        'year_of_ici_treatment_start': 'tx_year', \n",
    "        'progression_free_survival_months': 'PFS', \n",
    "        'overall_survival_months': 'OS',\n",
    "        'clinical_benefit_with_ici': 'clin_benefit',\n",
    "        'age_at_ici_start': 'age',\n",
    "        'ici_drug_class': 'drug_class',\n",
    "        'response_to_ici': 'response',\n",
    "        'stage_at_ici_start': 'stage',\n",
    "        'ecog_ps': 'ECOG',\n",
    "        'ici_line_of_treatment': 'tx_line'\n",
    "})\n",
    "print(data_clean.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Print the number of unique levels per column to identify if any abberant categories (eg. categories identified as unique due to differences in string case)\n",
    "\n",
    "#We note that there are 4 different levels fo the \"alive\" status, which will need to be collapsed into one category\n",
    "\n",
    "object_columns = data_clean.select_dtypes(include=['object']).columns\n",
    "for col in object_columns:\n",
    "    unique_levels = data_clean[col].unique()\n",
    "    print(f\"Unique levels for column '{col}': {unique_levels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restrict analysis to adults only (exclude kids)\n",
    "\n",
    "excluded_categories_age = ['0 - 5', '6 - 10', '11 - 15', '16 - 20']\n",
    "data_clean1 = data_clean[~data_clean['age'].isin(excluded_categories_age)]\n",
    "#print(data_clean2)\n",
    "\n",
    "#Exclude gynecological cancers and cancers with low counts/low responsiveness to checkpoint inhibitor therapy\n",
    "excluded_categories_cancer = ['Endometrial', 'Sarcoma', 'Breast', 'Ovarian', 'Mesothelioma', 'CNS', 'Unknown primary']\n",
    "data_clean2 = data_clean1[~data_clean1['cancer_type'].isin(excluded_categories_cancer)]\n",
    "\n",
    "#check variable levels properly excluded\n",
    "unique_levels2 = data_clean2['age'].unique()\n",
    "print(unique_levels2)\n",
    "\n",
    "unique_levels2 = data_clean2['cancer_type'].unique()\n",
    "print(unique_levels2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Map age into 10 year blocks\n",
    "mapping_age = {'21 - 25': '21 - 30', '26 - 30': '21 - 30', '31 - 35': '31 - 40', \n",
    "           '36 - 40': '31 - 40', '41 - 45': '41 - 50', '46 - 50': '41 - 50',\n",
    "           '51 - 55': '51 - 60', '56 - 60': '51 - 60',\n",
    "           '61 - 65': '61 - 70', '66 - 70': '61 - 70',\n",
    "           '71 - 75': '71 - 80', '76 - 80': '71 - 80', '81 -86': '81+', '86 - 90': '81+', '91 - 95': '81+'}\n",
    "\n",
    "# Regroup the categorical variable using the mapping\n",
    "data_clean2['age'] = data_clean2['age'].replace(mapping_age)\n",
    "\n",
    "mapping_cancer2 = { 'Colorectal':'LGI',\n",
    "              \n",
    "                  'SCLC': 'Lung',\n",
    "                  'Melanoma': 'Melanoma',\n",
    "                  'Hepatobiliary': 'UGI',\n",
    "                  'NSCLC': 'Lung',\n",
    "                  'Gastric': 'UGI',\n",
    "                  'Pancreatic': 'UGI',\n",
    "                  'Renal': 'GU',\n",
    "                  'Bladder': 'GU',\n",
    "                  'Esophageal': 'UGI',\n",
    "                  'Head & Neck': 'HNC',\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "data_clean2['cancer_type'] = data_clean2['cancer_type'].replace(mapping_cancer2)\n",
    "\n",
    "print(data_clean2)\n",
    "\n",
    "unique_levels2 = data_clean2['age'].unique()\n",
    "unique_levels2 = data_clean2['cancer_type'].unique()\n",
    "print(unique_levels2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarize categorical variables in summary tables\n",
    "\n",
    "categorical_variables = ['age', 'progression', 'vital_status', 'drug_class','msi_type','best_response', 'response', 'clin_benefit','stage','tx_line', 'sex', 'cancer_type']\n",
    "\n",
    "# Generate summary tables for each categorical variable\n",
    "summary_tables = {}\n",
    "for variable in categorical_variables:\n",
    "    summary_table = data_clean2[variable].value_counts().reset_index()\n",
    "    summary_table.columns = [variable, 'Count']\n",
    "    summary_tables[variable] = summary_table\n",
    "\n",
    "# Print summary tables\n",
    "for variable, table in summary_tables.items():\n",
    "    print(f\"\\nSummary table for '{variable}':\")\n",
    "    print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#colate 'vital_status' values\n",
    "data_clean2['vital_status'] = data_clean2['vital_status'].str.lower().replace({'alive ': 'alive', 'alive': 'alive'})\n",
    "\n",
    "# Generate summary table for 'vital_status'\n",
    "summary_table = data_clean2['vital_status'].value_counts().reset_index()\n",
    "summary_table.columns = ['vital_status', 'Count']\n",
    "\n",
    "print(summary_table)\n",
    "\n",
    "\n",
    "#colate 'drug class' values given low counts for CTLA4\n",
    "data_clean2['drug_class'] = data_clean2['drug_class'].str.lower().replace({'combo': 'ctla4/combo'})\n",
    "data_clean2['drug_class'] = data_clean2['drug_class'].str.lower().replace({'combo': 'ctla4/combo'})\n",
    "data_clean2['drug_class'] = data_clean2['drug_class'].str.lower().replace({'ctla-4': 'ctla4/combo'})\n",
    "\n",
    "# Generate summary table for 'drug_class'\n",
    "summary_table = data_clean2['drug_class'].value_counts().reset_index()\n",
    "summary_table.columns = ['drug_class', 'Count']\n",
    "\n",
    "print(summary_table)\n",
    "\n",
    "#colate stage variable\n",
    "\n",
    "data_clean2['stage'] = data_clean2['stage'].str.lower().replace({'i': 'local'})\n",
    "data_clean2['stage'] = data_clean2['stage'].str.lower().replace({'ii': 'local'})\n",
    "data_clean2['stage'] = data_clean2['stage'].str.lower().replace({'iii': 'local'})\n",
    "data_clean2['stage'] = data_clean2['stage'].str.lower().replace({'iv': 'advanced'})\n",
    "# Generate summary table for 'drug_class'\n",
    "summary_table = data_clean2['stage'].value_counts().reset_index()\n",
    "summary_table.columns = ['stage', 'Count']\n",
    "\n",
    "print(summary_table)\n",
    "\n",
    "#colate ecog variable\n",
    "\n",
    "# Convert the values in the 'ECOG' column to strings\n",
    "data_clean2['ECOG'] = data_clean2['ECOG'].astype(str)\n",
    "\n",
    "# Replace specific values in the 'ECOG' column\n",
    "data_clean2['ECOG'] = data_clean2['ECOG'].replace({'2.0': '2+', '3.0': '2+', '4.0': '2+'})\n",
    "\n",
    "# Create a summary table showing the counts of each unique value in the 'ECOG' column\n",
    "summary_table = data_clean2['ECOG'].value_counts().reset_index()\n",
    "\n",
    "# Rename the columns of the summary table\n",
    "summary_table.columns = ['ECOG', 'Count']\n",
    "\n",
    "print(summary_table)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_correlation = ['OS', \"PFS\", \"nlr\", \"tmb_mutations_mb\", \"id\"] #I included ID as a quality control.\n",
    "selected_columns = data_clean2[columns_correlation]\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "correlation=selected_columns.corr().abs()\n",
    "heatmap = sns.heatmap(correlation, vmin=0, vmax=1, annot=True, cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check correlations between vital status, and other continuous variables in the model. #ID included as quality check, which has a \n",
    "#slightly higher correlation than I would have expected.\n",
    "\n",
    "# Compute direct correlations of selected features with the target variable\n",
    "correlations_original = data_clean2.corrwith(data_clean2['vital_status'],method='spearman')\n",
    "# Sort them according the strength of correlations.\n",
    "correlations_index = correlations_original.abs().sort_values(kind=\"quicksort\", ascending=False).index\n",
    "# Use index to sort correlations\n",
    "correlations = correlations_original[correlations_index]\n",
    "# Display correlation value from original dataframe\n",
    "correlations_df = pd.DataFrame(correlations, index=correlations.index, columns=['Correlation'])\n",
    "# Print correlations\n",
    "display(correlations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.preprocessing import OneHotEncoder\n",
    "\n",
    "# Convert specific columns to categorical dtype\n",
    "\n",
    "columns_to_convert = ['response', 'clin_benefit', 'cancer_type', 'sex', 'cohort', 'drug_class', 'progression']  # List of columns to convert\n",
    "data_clean2[columns_to_convert] = data_clean2[columns_to_convert].astype('category')\n",
    "\n",
    "# Apply OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "data_encoded = encoder.fit_transform(data_clean2[columns_to_convert])\n",
    "\n",
    "# Concatenate the encoded DataFrame with data_clean\n",
    "data_clean_encoded = pd.concat([data_clean2.drop(columns=columns_to_convert), data_encoded], axis=1)\n",
    "\n",
    "print(data_clean_encoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing data was handled by complete case analysis\n",
    "data_complete = data_clean_encoded.dropna()\n",
    "display(data_complete)\n",
    "data_complete.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_complete.describe() #treating all as continuous. Still need to store tx_year as a year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of continuous variables\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "n_bins = int(np.ceil(np.sqrt(data_complete.shape[0])))\n",
    "\n",
    "# Plot histogram for nlr\n",
    "# Set figure size\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(data_complete['nlr'], bins=n_bins, color=\"black\")\n",
    "plt.xlabel('Neutrophil to Lymphocyte Ratio')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Calculates number of bins for the histograms by taking the square root of the number of data points (comes out as 50)\n",
    "n_bins = int(np.ceil(np.sqrt(data_complete.shape[0])))\n",
    "\n",
    "# Plot histogram for nlr\n",
    "# Set figure size\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(data_complete['tmb_mutations_mb'], bins=n_bins, color=\"black\")\n",
    "plt.xlabel('Tumor Mutational Burden')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Ydata_Profiling Summary\n",
    "from ydata_profiling import ProfileReport\n",
    "profile=ProfileReport(data_complete, title=\"Merged Summary\")\n",
    "#profile.to_notebook_iframe()\n",
    "profile.to_file(\"merged_summary_report.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaplan Meier Survival Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sksurv.nonparametric import kaplan_meier_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert vital status variable to 1=dead and 0=alive\n",
    "\n",
    "print(data_complete['vital_status'].unique())\n",
    "\n",
    "data_complete['vital_status'] = data_complete['vital_status'].replace({'dead': 1, 'alive': 0}).astype('float64')\n",
    "print(data_complete['vital_status'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "#This was done mainly to permit CoxPH analysis, as there were issues with collinearity due to the large number of dummy encoded variables.\n",
    "\n",
    "# Initialize the encoder\n",
    "encoder = OrdinalEncoder()\n",
    "\n",
    "# List of variables to encode\n",
    "variables_to_encode = ['age', 'best_response', 'stage', 'ECOG', 'msi_type', 'tx_line']\n",
    "\n",
    "# Fit and transform all variables at once\n",
    "encoded_vars = encoder.fit_transform(data_complete[variables_to_encode])\n",
    "\n",
    "# Add the encoded variables back to the DataFrame\n",
    "for i, var in enumerate(variables_to_encode):\n",
    "    data_complete[f'{var}_encoded'] = encoded_vars[:, i]\n",
    "\n",
    "# Print the DataFrame to verify the encoding\n",
    "print(data_complete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#I will need to scale the data later in the project so I will do that here\n",
    "\n",
    "cancer_scaled_all = data_complete.copy()\n",
    "\n",
    "# Normalise the data from 0 to 1\n",
    "scaler = StandardScaler()\n",
    "\n",
    "cancer_columns_to_scale = [col for col in cancer_scaled_all.columns if col in ['nlr', 'tmb_mutations_mb']]\n",
    "\n",
    "# Fit the scaler to the data and transform the selected columns\n",
    "cancer_scaled_all[cancer_columns_to_scale] = scaler.fit_transform(cancer_scaled_all[cancer_columns_to_scale])\n",
    "\n",
    "display(cancer_scaled_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into test and train\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "test_size = int(0.15 * len(cancer_scaled_all))\n",
    "\n",
    "# Shuffle the indices\n",
    "indices = np.arange(len(cancer_scaled_all))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Split indices into training and testing sets\n",
    "train_df = cancer_scaled_all.iloc[:-test_size]\n",
    "test_df = cancer_scaled_all.iloc[-test_size:]\n",
    "display(train_df)\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overall Survival\n",
    "import matplotlib.pyplot as plt\n",
    "train_df['vital_status'] = train_df['vital_status'].replace({'dead': 1, 'alive': 0}).astype('float64')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [6, 4]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "timedeath = train_df['OS']\n",
    "timedeath[train_df['vital_status'] < 1.] #patients identified as died when vital status is 0.\n",
    "\n",
    "statusdeath = [x > 0. for x in train_df['vital_status']] #creates a boolean whether whether x is greater than 0 or not\n",
    "time, survival_prob = kaplan_meier_estimator(statusdeath, timedeath)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.step(time, survival_prob, where=\"post\", color=\"black\")\n",
    "plt.ylabel(\"est. probability of survival $\\hat{S}(t)$\")\n",
    "plt.xlabel(\"time $t$ (in months)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cox Proportional Hazards Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (de Kamps, 2023)\n",
    "import numpy as np\n",
    "\n",
    "# Random seed for the future\n",
    "np.random.seed(42)\n",
    "\n",
    "#creates an numpy array by zipping together OS and vital status into a tupple, converting to list, and then array\n",
    "dfy = np.array(list(zip(statusdeath, timedeath)), dtype=[('Status', '?'), ('Survival_in_months', '<f8')])\n",
    "print(dfy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"response\" variable not included in model, as \"best-response\" provided more granular data, and provide overlapping information\n",
    "\n",
    "#Final model included sex (2 levels), drug_class (2 levels), progression (2 levels), cancer_type (6 levels), overall survival (continuous), vital_status(2 levels)\n",
    "#neutrophil to lymphocyte ratio, tumor mutational burden, age, best response to treatment, cancer stage, functional status (ECOG0)\n",
    "#and microsatellite instability (3 levels)\n",
    "\n",
    "columns_to_select = train_df.filter(like='sex').columns.tolist()\n",
    "columns_to_select += train_df.filter(like='drug_class').columns.tolist()\n",
    "columns_to_select += train_df.filter(like='progression').columns.tolist()\n",
    "columns_to_select += train_df.filter(like='cancer_type').columns.tolist()\n",
    "columns_to_select += ['OS', 'vital_status', 'nlr','tmb_mutations_mb', 'age_encoded', \n",
    "                      'best_response_encoded', 'stage_encoded', 'ECOG_encoded',\n",
    "                      'msi_type_encoded'\n",
    "                      ]\n",
    "\n",
    "# Select the desired columns\n",
    "selected_columns = train_df[columns_to_select]\n",
    "\n",
    "# View the selected columns\n",
    "print(selected_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop vital status and overall survival from dfx\n",
    "\n",
    "dfx = selected_columns.drop(['vital_status', 'OS'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "\n",
    "#I want my dfx dataframe to contain all variables except vital_status and OS\n",
    "\n",
    "dfx = selected_columns.drop(['vital_status', 'OS'], axis=1, inplace=False)\n",
    "\n",
    "estimator = CoxPHSurvivalAnalysis()\n",
    "estimator.fit(dfx, dfy)\n",
    "\n",
    "#This gives the C statistic\n",
    "estimator.score(dfx, dfy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of dfx:\", dfx.shape)\n",
    "print(\"Shape of dfy:\", dfy.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(estimator.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform 3 fold CV validation, to give a mean C statistic across 3 folds\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(estimator, dfx, dfy, cv=3)\n",
    "print(f\"Mean score CoxPH: {scores.mean():.3f} (std: {scores.std():.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of Negative Partial Log Likelihood as  Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "# Adapted from PÃ¶lsterl (2023); de Kamps (2023)\n",
    "\n",
    "#The code below are a number of helper functions utilized by Mbotwa et al \n",
    "\n",
    "#Implement an optimizer for CoxPH\n",
    "\n",
    "class CoxPHOptimizer:\n",
    "    \"\"\"Negative partial log-likelihood of Cox proportional hazards model.\"\"\"\n",
    "\n",
    "    def __init__(self, X, event, time, alpha, ties):\n",
    "        \"\"\"Initialize the model.\n",
    "\n",
    "        Args:\n",
    "            X (pandas.DataFrame): The dataset.\n",
    "            event (array): List of events.\n",
    "            time (array): List of times.\n",
    "            alpha (array): List of L2 penalties for each coefficient.\n",
    "            ties (string): Which method to use for ties. One of 'breslow' or 'efron'.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If ties is not one of 'breslow' or 'efron'.\n",
    "        \"\"\"\n",
    "        # Sort descending\n",
    "        o = np.argsort(-time, kind=\"mergesort\")\n",
    "        self.x = torch.tensor(X[o, :], dtype=torch.float)\n",
    "        self.event = event[o]\n",
    "        self.time = time[o]\n",
    "        self.alpha = alpha\n",
    "        # Method to handle ties\n",
    "        if ties not in (\"breslow\", \"efron\"):\n",
    "            raise ValueError(\"ties must be one of 'breslow', 'efron'\")\n",
    "        ties = \"breslow\"\n",
    "        self._is_breslow = ties == \"breslow\"\n",
    "\n",
    "#Function to compute the negative partial log likelihood (PLL)\n",
    "#Returns average PLL per event\n",
    "\n",
    "    def nlog_likelihood(self, w=None, model=None):\n",
    "        \"\"\"Compute negative partial log-likelihood.\n",
    "\n",
    "        Args:\n",
    "            w (array, shape = (n_features,), optional): Estimate of coefficients. Defaults to None.\n",
    "            model (torch.nn.Module, optional): The NN model. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            float: Average negative partial log-likelihood (NPLL per event).\n",
    "        \"\"\"\n",
    "        time = self.time\n",
    "        n_samples = self.x.shape[0]\n",
    "        breslow = self._is_breslow\n",
    "        xw = []\n",
    "                \n",
    "        if model is not None:\n",
    "            for i in range(n_samples):\n",
    "                # Computes risk score for each sample using the NN model\n",
    "                # and appends it to the list xw, containing the risk scores for all samples.\n",
    "                xw.append(model.forward(self.x[i]))\n",
    "                # self.update(model)\n",
    "        else:\n",
    "            #If no model we perform a matrix-vector product using the feature matrix self.x\n",
    "            #and a vector of the weights (in a linear combination)\n",
    "            #xw is the risk score\n",
    "            xw = self.x@w\n",
    "\n",
    "        #loss, risk_set and k(the index) are set to zero\n",
    "        loss = 0\n",
    "        risk_set = 0\n",
    "        k = 0\n",
    "        \n",
    "        #Iterate until we reach the total number of samples in the dataset\n",
    "        while k < n_samples:\n",
    "            ti = time[k] #time of event for k index\n",
    "            numerator = 0 #initialize variable for samples with events\n",
    "            n_events = 0 #of events occuring at same time\n",
    "            risk_set2 = 0 # samples without events at same time\n",
    "            while k < n_samples and ti == time[k]: #iterate over samples with same event time\n",
    "                if self.event[k]: #if sample has event\n",
    "                    numerator = numerator + xw[k] #risk score added to numerator\n",
    "                    risk_set2 = risk_set2 + torch.exp(xw[k]) #exponential of risk score added to risk_set2, accumulates scores for samples without event at same time\n",
    "                    n_events = n_events + 1 #increment number of events happening at same time\n",
    "                else:\n",
    "                    risk_set = risk_set + torch.exp(xw[k]) #if sample does not have event, added to risk_set\n",
    "                k = k + 1\n",
    "\n",
    "#This part of the code defines different methods for dealing with ties (Breslow and Efron depending on what is selected)\n",
    "            if n_events > 0:\n",
    "                if breslow:\n",
    "                    risk_set = risk_set + risk_set2\n",
    "                    loss = loss - (numerator - n_events * torch.log(risk_set)) / n_samples\n",
    "                else:\n",
    "                    numerator = numerator/n_events\n",
    "                    for _ in range(n_events):\n",
    "                        risk_set = risk_set + risk_set2 / n_events\n",
    "                        loss = loss - (numerator - torch.log(risk_set)) / n_samples\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    # function adapted for torch tensors instead of numpy arrays.\n",
    "    def update(self, model, offset=0):\n",
    "        \"\"\"Compute and updates gradient and Hessian matrix.\n",
    "        \n",
    "        Args:\n",
    "            model (torch.nn.Module): The NN model.\n",
    "            offset (float, optional): The offset. Defaults to 0.\n",
    "        \"\"\"\n",
    "        time = self.time\n",
    "        x = self.x\n",
    "        breslow = self._is_breslow\n",
    "        n_samples, n_features = x.shape\n",
    "        \n",
    "        # Compute risk score for each sample using the NN model\n",
    "        # Everything else in this function is the same as the original code\n",
    "        exp_xw = []\n",
    "        for i in range(n_samples):\n",
    "            exp_xw.append(torch.exp(offset + model.forward(torch.tensor(x[i], dtype=torch.float32))).item())\n",
    "\n",
    "        gradient = torch.zeros((1, n_features), dtype=torch.float64)\n",
    "        hessian = torch.zeros((n_features, n_features), dtype=torch.float64)\n",
    "\n",
    "        inv_n_samples = 1. / n_samples\n",
    "        risk_set = 0\n",
    "        risk_set_x = torch.zeros((1, n_features), dtype=torch.float64)\n",
    "        risk_set_xx = torch.zeros((n_features, n_features), dtype=torch.float64)\n",
    "        k = 0\n",
    "        \n",
    "        # Iterate time in descending order\n",
    "        while k < n_samples:\n",
    "            ti = time[k]\n",
    "            n_events = 0\n",
    "            numerator = 0\n",
    "            risk_set2 = 0\n",
    "            risk_set_x2 = torch.zeros_like(risk_set_x)\n",
    "            risk_set_xx2 = torch.zeros_like(risk_set_xx)\n",
    "            while k < n_samples and ti == time[k]:\n",
    "                # Preserve 2D shape of row vector\n",
    "                xk = x[k:k + 1]\n",
    "\n",
    "                # Outer product\n",
    "                xx = torch.matmul(xk.T, xk)\n",
    "\n",
    "                if self.event[k]:\n",
    "                    numerator += xk\n",
    "                    risk_set2 += exp_xw[k]\n",
    "                    risk_set_x2 += exp_xw[k] * xk\n",
    "                    risk_set_xx2 += exp_xw[k] * xx\n",
    "                    n_events += 1\n",
    "                else:\n",
    "                    risk_set += exp_xw[k]\n",
    "                    risk_set_x += exp_xw[k] * xk\n",
    "                    risk_set_xx += exp_xw[k] * xx\n",
    "                k += 1\n",
    "\n",
    "            if n_events > 0:\n",
    "                if breslow:\n",
    "                    risk_set += risk_set2\n",
    "                    risk_set_x += risk_set_x2\n",
    "                    risk_set_xx += risk_set_xx2\n",
    "\n",
    "                    z = risk_set_x / risk_set\n",
    "                    gradient -= (numerator - n_events * z) * inv_n_samples\n",
    "\n",
    "                    a = risk_set_xx / risk_set\n",
    "                    # outer product\n",
    "                    b = torch.matmul(z.T, z)\n",
    "\n",
    "                    hessian += n_events * (a - b) * inv_n_samples\n",
    "                else:\n",
    "                    numerator /= n_events\n",
    "                    for _ in range(n_events):\n",
    "                        risk_set += risk_set2 / n_events\n",
    "                        risk_set_x += risk_set_x2 / n_events\n",
    "                        risk_set_xx += risk_set_xx2 / n_events\n",
    "\n",
    "                        z = risk_set_x / risk_set\n",
    "                        gradient -= (numerator - z) * inv_n_samples\n",
    "\n",
    "                        a = risk_set_xx / risk_set\n",
    "                        # outer product\n",
    "                        b = torch.matmul(z.T, z)\n",
    "\n",
    "                        hessian += (a - b) * inv_n_samples\n",
    "\n",
    "        self.gradient = gradient.ravel()\n",
    "        self.hessian = hessian\n",
    "    \n",
    "    def zero(self):\n",
    "        \"\"\"Reset gradient and Hessian matrix.\"\"\"\n",
    "        self.gradient = 0\n",
    "        self.hessian = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure only the columns with the predictors of interest are included (columns_to_select previously defined)\n",
    "train_df = train_df[columns_to_select]\n",
    "print(train_df.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (de Kamps, 2023)\n",
    "\n",
    "%pip install lifelines\n",
    "\n",
    "from lifelines import CoxPHFitter\n",
    "\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "#This is another method of fitting a COXPH model\n",
    "\n",
    "cph_IO = CoxPHFitter().fit(train_df, 'vital_status', 'OS')\n",
    "cph_IO.print_summary(columns=[\"coef\", \"se(coef)\", \"p\"])\n",
    "\n",
    "# Store the coefficients\n",
    "cph_IO_coef = cph_IO.summary['coef']\n",
    "\n",
    "print(cph_IO.concordance_index_)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#I experimented a bit with different penelties. \n",
    "#It seems like no penelty is better, some exampels of lasso (l1) and ridge (l2) penalty\n",
    "cph_IOa = CoxPHFitter(penalizer=0.01, l1_ratio=1).fit(train_df, 'vital_status', 'OS') # Set the penalizer value for L1 penalty\n",
    "cph_IOa.print_summary(columns=[\"coef\", \"se(coef)\", \"p\"])\n",
    "\n",
    "# Store the coefficients\n",
    "cph_IO_coef = cph_IOa.summary['coef']\n",
    "\n",
    "cph_IOb = CoxPHFitter(penalizer=0.01, l1_ratio=0).fit(train_df, 'vital_status', 'OS') # Set the penalizer value for L2 penalty and l1_ratio=0 for Ridge penalty\n",
    "cph_IOb.print_summary(columns=[\"coef\", \"se(coef)\", \"p\"])\n",
    "\n",
    "# Store the coefficients\n",
    "cph_IO_coef = cph_IOb.summary['coef']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test CoxPH Model in the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "# Extract the event times and vital status from the testing dataset\n",
    "event_times = test_df['OS']\n",
    "vital_status = test_df['vital_status']\n",
    "\n",
    "# Calculate the C-statistic\n",
    "c_index = concordance_index(event_times, cph_IO.predict_partial_hazard(test_df))\n",
    "print(\"C-statistic:\", c_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survival Analysis with CoxNNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from de Kamps (2023); Ching et al. (2018)- The authors of this code referenced de Kamps 2023, but I have not confidently identified\n",
    "#the primary source.\n",
    "\n",
    "torch.autograd.set_detect_anomaly = True\n",
    "\n",
    "class CoxNNet(torch.nn.Module):\n",
    "    \"\"\"CoxNNet model for survival analysis.\"\"\"\n",
    "\n",
    "    #Define constructor for initialization of the model\n",
    "    def __init__(self, df, event, time, coefficients=None):\n",
    "        \"\"\"Initialize the CoxNNet model.\n",
    "\n",
    "        Args:\n",
    "            df (pandas.DataFrame): Dataframe containing the covariates and the event and time columns.\n",
    "            event (str): Name of the event column. \n",
    "            time (str): Name of the time column.\n",
    "            coefficients (torch.Tensor, optional): Initial coefficients. Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.covariates = [x for x in df.columns if x != event and x != time]\n",
    "        self.df = df\n",
    "        self.dfx = df[self.covariates].to_numpy(dtype=float)\n",
    "        self.events = df[event].to_numpy(dtype=float)\n",
    "        self.times = df[time].to_numpy(dtype=float)\n",
    "        \n",
    "        n_covariates = len(self.covariates)\n",
    "        \n",
    "        # If no coefficients are passed in, we initialize them randomly\n",
    "        if coefficients == None:\n",
    "            # Set random seed for reproducibility\n",
    "            torch.manual_seed(42)\n",
    "            self.coefficients = torch.randn(n_covariates, requires_grad=True, dtype=torch.float)\n",
    "        # Otherwise we use the coefficients passed in\n",
    "        else:\n",
    "            self.coefficients = coefficients.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        # This line is used in coxpy to initialize the alphas\n",
    "        alphas = torch.zeros(self.dfx.shape[1])\n",
    "        \n",
    "        self.opt = CoxPHOptimizer(self.dfx, self.events, self.times, alphas, 'breslow')\n",
    "\n",
    "    def forward(self, model=None):\n",
    "        \"\"\"Calculate the log-likelihood over the entire dataset.\n",
    "        This is the function that is called when we call net.forward().\n",
    "        Important: this is where we can pass in a NN model.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module, optional): NN model to use. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            float: Log-likelihood over the entire dataset.\n",
    "        \"\"\"\n",
    "        # sksurv calculates the log likelihood /event and puts a minus sign in front, we undo that\n",
    "        total = -self.df.shape[0]*self.opt.nlog_likelihood(self.coefficients, model)\n",
    "        return total\n",
    "\n",
    "# Simply testing to make sure the code works\n",
    "  \n",
    "\n",
    "# Generate a tensor. THis needs to equal the number of covariates excluding vital_status and OS\n",
    "num_features = 15\n",
    "\n",
    "# Generate a tensor with random values for the coefficients, matching the number of features\n",
    "c = torch.randn(num_features)\n",
    "#c = torch.randn(26)\n",
    "\n",
    "net = CoxNNet(train_df, 'vital_status', 'OS', coefficients=c)\n",
    "loss = net.forward()\n",
    "print(\"Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent to Maximize the Partial Log Likelihood in CoxNNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from de Kamps (2023)\n",
    "def gradient_descent(df, event, time, lr, n_epoch, coefficients=None):\n",
    "    \"\"\"Performs gradient descent on the Cox model.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Dataframe containing the covariates and the event and time columns.\n",
    "        event (str): Name of the event column.\n",
    "        time (str): Name of the time column.\n",
    "        lr (float): Learning rate.\n",
    "        n_epoch (int): Number of epochs (iterations to run).\n",
    "        coefficients (torch.Tensor, optional): Initial coefficients. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The trained CoxNNet model (with updated coefficients).\n",
    "    \"\"\"\n",
    "    \n",
    "    net = CoxNNet(df, event, time, coefficients)\n",
    "    loss = net.forward() #calculate initial loss by the forward method\n",
    "    print(\"Initial Loss:\", loss.item())\n",
    "    \n",
    "    # Iterate n_epoch times\n",
    "    for _ in range(n_epoch):\n",
    "        # Updates the coefficients using autograd\n",
    "        loss = net.forward()\n",
    "        loss.backward() #gradient of loss computed using backpropogation\n",
    "        with torch.no_grad():\n",
    "            # Use the specified learning rate\n",
    "            net.coefficients += net.coefficients.grad * lr #coefficnets updated using GD\n",
    "            net.coefficients.grad.zero_() #zeroed to prevent accumulation\n",
    "    loss = net.forward()\n",
    "    \n",
    "    # Print the final loss and the updated coefficients\n",
    "    print(\"Final Loss:\", loss.item())\n",
    "    print(\"Coefficients:\", net.coefficients.detach().numpy())\n",
    "    return net\n",
    "\n",
    "#The two digits (0.0001, 4000)refer to the learning rate, and the number of epochs\n",
    "cancer_net = gradient_descent(train_df, 'vital_status', 'OS', 0.0001, 4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent generalizable to other neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the NN using gradient descent\n",
    "#in this function, a model parameter is explicitly passed as an argument in the function\n",
    "\n",
    "def train_model(df, event, time, model, lr, n_epoch, output=False):\n",
    "    \"\"\"Trains a NN model using gradient descent.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Dataframe containing the covariates and the event and time columns.\n",
    "        event (str): Name of the event column.\n",
    "        time (str): Name of the time column.\n",
    "        model (_type_): NN model.\n",
    "        lr (float): Learning rate.\n",
    "        n_epoch (int): Number of epochs (iterations to run).\n",
    "        output (bool, optional): Whether to print the loss before and after training. Defaults to True.\n",
    "        \n",
    "    Returns:\n",
    "        float: The final loss.\n",
    "    \"\"\"\n",
    "    net = CoxNNet(df, event, time)\n",
    "    losses = []\n",
    "    \n",
    "    # Print the initial loss before training\n",
    "    if output:\n",
    "        print(\"Initial Loss:\", net.forward(model).item())\n",
    "    \n",
    "    # Train the model\n",
    "    for _ in range(n_epoch):\n",
    "        # Compute the loss using the CoxNNet class and the NN model\n",
    "        loss = net.forward(model)\n",
    "        loss.backward()        \n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Update the weights using gradient descent\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param += param.grad * lr\n",
    "            model.zero_grad()\n",
    "    \n",
    "    # Print the final loss\n",
    "    if output:\n",
    "        print(\"Final Loss:\", net.forward(model).item())\n",
    "        \n",
    "        # Produce a plot of the loss over the epochs\n",
    "        losses = [x.detach().numpy() for x in losses]\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(range(1, n_epoch+1), losses, color='black')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Negative Partial Log-Likelihood Loss')\n",
    "        plt.show()\n",
    "    \n",
    "    # Return the final PLE loss of the final trained model\n",
    "    return losses[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function to calculate the C-statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#C statistic assesses the number of concordant, discordant, and tied pairs. \n",
    "#Ratio of sum of concordant pairs + half tied pair/total pairs\n",
    "\n",
    "def concordance_statistic(risks, events, times, output=True):\n",
    "    \"\"\"Calculates the c-statistic for a given set of predicted risks and actual events and times.\n",
    "\n",
    "    Args:\n",
    "        risks (array): List of predicted risks.\n",
    "        events (array): List of event indicators.\n",
    "        times (array): List of survival times.\n",
    "\n",
    "    Returns:\n",
    "        float: c-statistic.\n",
    "    \"\"\"\n",
    "    valid_pairs = 0\n",
    "    concordant_pairs = 0\n",
    "    discordant_pairs = 0\n",
    "    tied_pairs = 0\n",
    "    \n",
    "    # Convert risks tensor to NumPy array\n",
    "    if isinstance(risks, torch.Tensor):\n",
    "        risks = risks.detach().cpu().numpy()\n",
    "    \n",
    "    # Sort risks, events, and times based on the times\n",
    "    sorted_indices = np.argsort(times)\n",
    "    risks = np.array(risks)[sorted_indices]\n",
    "    events = np.array(events)[sorted_indices]\n",
    "    times = np.array(times)[sorted_indices]\n",
    "\n",
    "    n_samples = len(risks)\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i + 1, n_samples):\n",
    "            if times[i] != times[j]:\n",
    "                if events[i] and (times[i] < times[j]):\n",
    "                    valid_pairs += 1\n",
    "                    if risks[i] > risks[j]:\n",
    "                        concordant_pairs += 1\n",
    "                    elif risks[i] < risks[j]:\n",
    "                        discordant_pairs += 1\n",
    "                    else:\n",
    "                        tied_pairs += 1\n",
    "\n",
    "                elif events[j] and (times[j] < times[i]):\n",
    "                    valid_pairs += 1\n",
    "                    if risks[j] > risks[i]:\n",
    "                        concordant_pairs += 1\n",
    "                    elif risks[j] < risks[i]:\n",
    "                        discordant_pairs += 1\n",
    "                    else:\n",
    "                        tied_pairs += 1\n",
    "    \n",
    "    # We can optionally produce a plot of the risk scores over time\n",
    "    # This is useful for visualising the risk scores\n",
    "    # Ideally, we would like to see the risk scores of the events (i.e. 1) higher than the non-events (i.e. 0)\n",
    "    # This is because the risk scores are the log-hazard ratios\n",
    "    if output:\n",
    "        # Plots the risk scores over time\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        colors = np.where(events, 'orange', 'blue')\n",
    "\n",
    "        for color, label in zip(['orange', 'blue'], ['Event', 'Non-Event']):\n",
    "            mask = (colors == color)\n",
    "            plt.scatter(times[mask], np.array(risks)[mask], alpha=0.3, color=color, label=label)\n",
    "\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Risk')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    # Note we do not actually need to use discordant_pairs in the calculation\n",
    "    # Since we already increment valid_pairs when we encounter a discordant pair\n",
    "    return (concordant_pairs + 0.5 * tied_pairs) / valid_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the C-statistic function in the CoxNNet model\n",
    "\n",
    "def get_concordance_statistic(df, event, time, net, output=True):\n",
    "    \"\"\"Calculates the c-statistic for a given dataset for the CoxNNet model.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Dataframe containing the covariates and the event and time columns.\n",
    "        event (str): Name of the event column.\n",
    "        time (str): Name of the time column.\n",
    "        net (torch.nn.Module): CoxNNet model.\n",
    "\n",
    "    Returns:\n",
    "        float: c-statistic.\n",
    "    \"\"\"\n",
    "    # Calculate the predicted risk for each individual in the dataset\n",
    "    risks = np.matmul(net.dfx, net.coefficients.detach().numpy())\n",
    "    \n",
    "    return concordance_statistic(risks, df[event].to_numpy(), df[time].to_numpy(), output)\n",
    "\n",
    "print(\"Base Cancer Score:\", get_concordance_statistic(train_df, 'vital_status', 'OS', cancer_net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C statistic helper functions generalizable to other neural networks\n",
    "\n",
    "def get_concordance_statistic_nn(df, event, time, model, output=True):\n",
    "    \"\"\"Calculates the c-statistic for a given dataset using a NN model to compute predicted risks.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Dataframe containing the covariates and the event and time columns.\n",
    "        event (str): Name of the event column.\n",
    "        time (str): Name of the time column.\n",
    "        model (torch.nn.Module): NN model.\n",
    "        output (bool, optional): Whether to print the loss before and after training. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        float: c-statistic.\n",
    "    \"\"\"\n",
    "    # Calculate the predicted risk for each individual in the dataset using the NN\n",
    "    x = torch.tensor(df[[col for col in df.columns if col not in [event, time]]].values, dtype=torch.float)    \n",
    "    with torch.no_grad():\n",
    "        predicted_risk = np.array([model.forward(xi).item() for xi in x])\n",
    "    \n",
    "    # Compute the c-statistic\n",
    "    return concordance_statistic(predicted_risk, df[event].to_numpy(), df[time].to_numpy(), output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Linear Model with Fixed Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple NN with no hidden layers, essentially the same as a CoxPH model. Unable to capture non linear relationships\n",
    "\n",
    "class FixedWeightLinear(torch.nn.Module):\n",
    "    \"\"\"Simple NN with no hidden layers that simply computes the dot product of the input and the weights.\n",
    "    This is essentially the same as the CoxPH model.\n",
    "    We do not train this model, we simply use it to compute the predicted risk for each individual in the dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, weights):\n",
    "        \"\"\"Initializes the FWL model.\n",
    "\n",
    "        Args:\n",
    "            weights (torch.Tensor): Weights to use for the dot product.\n",
    "        \"\"\"\n",
    "        #\"super\" allows you to call methods from the parent class\n",
    "        super(FixedWeightLinear, self).__init__()\n",
    "        # Initialize the weights as a torch.nn.Parameter\n",
    "        self.weights = torch.nn.Parameter(weights.clone().detach().requires_grad_(True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Simply multiply the weights by the input.\n",
    "\n",
    "        Args:\n",
    "            x (array): List of covariates for a single individual.\n",
    "\n",
    "        Returns:\n",
    "            float: Predicted risk.\n",
    "        \"\"\"\n",
    "        return torch.matmul(x, self.weights)\n",
    "    \n",
    "# Initialize the FWL model with the learned coefficients\n",
    "cancer_fwl = FixedWeightLinear(cancer_net.coefficients)\n",
    "\n",
    "# Compute the c-statistic using the FWL model\n",
    "print(\"FWL Cancer Score:\", get_concordance_statistic_nn(train_df, 'vital_status', 'OS', cancer_fwl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Weights Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class lets us use random weights for each feature\n",
    "class VariableWeightLinear(FixedWeightLinear):\n",
    "    \"\"\"Simple NN with no hidden layers that simply computes the dot product of the input and the weights.\n",
    "    However, the weights are randomly initialized and then trained using gradient descent.\"\"\"\n",
    "    \n",
    "    def __init__(self, n):\n",
    "        \"\"\"Initializes the VWL model.\n",
    "\n",
    "        Args:\n",
    "            n (int): Number of features in the dataset.\n",
    "        \"\"\"\n",
    "        # Randomise weights\n",
    "        torch.manual_seed(42)\n",
    "        weights = torch.nn.Parameter(torch.randn(n, requires_grad=True, dtype=torch.float))\n",
    "        super(VariableWeightLinear, self).__init__(weights)\n",
    "        # Print these initial weights\n",
    "        print(\"Initial VWL\", self.weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Simply multiply the weights by the input.\n",
    "\n",
    "        Args:\n",
    "            x (array): List of covariates for a single individual.\n",
    "\n",
    "        Returns:\n",
    "            float: Predicted risk.\n",
    "        \"\"\"\n",
    "        # Again, simply multiply the weights by the input\n",
    "        return torch.matmul(x, self.weights)\n",
    "    \n",
    "# Initialize the VariableWeightLinear model\n",
    "n_covariates = len([x for x in train_df.columns if x != 'vital_status' and x != 'OS']) #covariates exclude vital_status and OS\n",
    "cancer_vwl = VariableWeightLinear(n_covariates)\n",
    "\n",
    "# Compute the c-statistic using the trained VWL model\n",
    "print(\"Initial VWL Cancer Score:\", get_concordance_statistic_nn(train_df, 'vital_status', 'OS', cancer_vwl, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the train_model function previously defined.\n",
    "cancer_vwl_loss = train_model(train_df, 'vital_status', 'OS', cancer_vwl, 0.0001, 4000, False)\n",
    "\n",
    "# Compute the c-statistic using the trained VWL model\n",
    "cancer_vwl_c = get_concordance_statistic_nn(train_df, 'vital_status', 'OS', cancer_vwl, False)\n",
    "\n",
    "# Print the trained weights and loss in format string\n",
    "print(\"Trained VWL Weights: {0} \\\n",
    "      \\nTrained VWL Loss: {1} \\\n",
    "      \\nTrained VWL Score {2}\"\n",
    "      .format(cancer_vwl.weights, cancer_vwl_loss, cancer_vwl_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights(weights):\n",
    "    \"\"\"Plots the distribution of the weights.\n",
    "\n",
    "    Args:\n",
    "        weights (torch.Tensor): Weights to plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(weights, bins=int(np.sqrt(len(weights))), color='black')\n",
    "    plt.xlabel('Weights')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "class SNN(torch.nn.Module):\n",
    "    \"\"\"Simple NN with one hidden layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size=None):\n",
    "        \"\"\"Initializes the SNN model.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Number of covariates.\n",
    "            hidden_size (int): Number of neurons in the hidden layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Randomise weights\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        super(SNN, self).__init__()\n",
    "        # Store the input and hidden sizes\n",
    "        self.input_size = input_size\n",
    "        # If no hidden size is specified, use the sqrt(input_size + 1) rounded up\n",
    "        if hidden_size is None:\n",
    "            self.hidden_size = np.sqrt(input_size + 1).astype(int) + 1\n",
    "        else:\n",
    "            self.hidden_size = hidden_size\n",
    "        \n",
    "        # Create the layers\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size) #creates first fully connected layer \n",
    "        #with specified input size and hidden size. Performs linear tranformation of data\n",
    "        self.relu = torch.nn.ReLU() #non linear activation function\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, 1) #produces a single output layer\n",
    "        \n",
    "        # Create the model\n",
    "        self.model = torch.nn.Sequential(self.fc1, self.relu, self.fc2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Predicts the risk for a single individual.\n",
    "\n",
    "        Args:\n",
    "            x (array): List of covariates for a single individual.\n",
    "\n",
    "        Returns:\n",
    "            float: Predicted risk.\n",
    "        \"\"\"\n",
    "        return self.model(x)\n",
    "    \n",
    "    def return_weights(self):\n",
    "        \"\"\"Plots the weights in a histogram.\"\"\"\n",
    "        \n",
    "        plot_weights(torch.cat([self.fc1.weight.view(-1), self.fc1.bias, self.fc2.weight.view(-1), self.fc2.bias]).detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Different Architectures of the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This allows us to test different architectures of the `NN` model\n",
    "def test_nn(df, event, time, model, lr, n_epoch, output=False):\n",
    "    \"\"\"Trains the model and prints the c-statistic.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Dataset to use.\n",
    "        event (str): Name of the event column.\n",
    "        time (str): Name of the time column.\n",
    "        model (torch.nn.Module): NN model.\n",
    "        lr (float): Learning rate.\n",
    "        n_epoch (int): Number of epochs.\n",
    "        output (bool, optional): Whether to print the output. Defaults to False.\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = train_model(df, event, time, model, lr, n_epoch, output)\n",
    "    score = get_concordance_statistic_nn(df, event, time, model, output)\n",
    "    \n",
    "    if output:\n",
    "        # Print format string with loss and score\n",
    "        param = \"\"\n",
    "        if type(model).__name__ == \"SNN\":\n",
    "            param = f\"[{model.input_size}, {model.hidden_size}, 1]\"\n",
    "        elif type(model).__name__ == \"GNN\": \n",
    "            param = model.layer_sizes\n",
    "        print(f\"{type(model).__name__}{param} \\n- Loss: {loss}\\n- Score: {score}\")\n",
    "    return loss, score\n",
    "\n",
    "#Note, for this dataset, the kernel \"dies\" if you set the EPOCH to 4000\n",
    "\n",
    "# Test the simple model on the cancer data set\n",
    "#The learning rate controls the step size taken during each parameter update\n",
    "input_size = train_df.shape[1] - 2  # Number of input features (excluding vital status and overall survival\n",
    "hidden_size = (np.sqrt(input_size + 1)).astype(int) + 1  # Number of hidden units. Taking the square root allows scaling of the number of the hidden units. Adding \"1\" ensures a minimum number obtained\n",
    "cancer_snn = SNN(input_size, hidden_size)\n",
    "\n",
    "#The final NN size will therefore be 15, sqrt(15)+1, 1 units\n",
    "test_nn(train_df, 'vital_status', 'OS', cancer_snn, 0.00001, 2000, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Different Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a loop to test different architectures (begin with 1 neuron in single hidden layer, increase in steps of 1 until 2 * input_size)\n",
    "cancer_snn_losses = []\n",
    "cancer_snn_scores = []\n",
    "\n",
    "# This allows us to test different architectures of the NN model\n",
    "# We do not want to add too many neurons because this can lead to overfitting OR \"dead neurons\"\n",
    "cancer_snn_hidden_sizes = [1 + i for i in range(input_size * 2)]\n",
    "\n",
    "# This stores the best model and its score\n",
    "cancer_snn_best = [None, 0, 0]\n",
    "\n",
    "# Loop through the different architectures\n",
    "for i in cancer_snn_hidden_sizes:\n",
    "    model = SNN(input_size, i)\n",
    "    loss, score = test_nn(train_df, 'vital_status', 'OS', model, 0.0001, 1000, False)\n",
    "    loss = loss.detach().numpy()\n",
    "    if score > cancer_snn_best[2]:\n",
    "        cancer_snn_best = [model, loss, score]\n",
    "    cancer_snn_losses.append(loss)\n",
    "    cancer_snn_scores.append(score)\n",
    "\n",
    "# Print the scores of the best architecture\n",
    "print(\"Best SNN: \\\n",
    "    \\n- Architecture: SNN[{0}, {1}, 1] \\\n",
    "    \\n- Loss: {2} \\\n",
    "    \\n- Score: {3}\"\n",
    "    .format(input_size, cancer_snn_best[0].hidden_size, cancer_snn_best[1], cancer_snn_best[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the losses\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(cancer_snn_hidden_sizes, cancer_snn_losses, label='Loss', color='black')\n",
    "plt.xlabel('Hidden Layer Size')\n",
    "plt.ylabel('Loss ')\n",
    "plt.show()\n",
    "\n",
    "# Plot the c-statistic scores\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(cancer_snn_hidden_sizes, cancer_snn_scores, label='C-Statistic', color='black')\n",
    "plt.xlabel('Hidden Layer Size')\n",
    "plt.ylabel('C-Statistic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a histogram with n_bins = size of data square root\n",
    "cancer_snn_best[0].return_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Generalization of Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    \"\"\"General NN with any number of hidden layers with any number of neurons.\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes):\n",
    "        \"\"\"Initializes the GNN model.\n",
    "\n",
    "        Args:\n",
    "            layer_sizes (array): List of the number of neurons in each layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Randomise weights\n",
    "        torch.manual_seed(42)\n",
    "        super(GNN, self).__init__()\n",
    "        self.layer_sizes = layer_sizes\n",
    "        # Create a list of layers\n",
    "        self.layers = torch.nn.ModuleList()        \n",
    "        # Create the layers based on the input sizes\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # Add a linear layer\n",
    "            self.layers.append(torch.nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            # Add a ReLU activation function after every linear layer except the last one\n",
    "            if i < len(layer_sizes) - 2:  # Don't add ReLU activation after the last linear layer\n",
    "                self.layers.append(torch.nn.ReLU())\n",
    "        # Create the model\n",
    "        self.model = torch.nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Predicts the risk for a single individual.\n",
    "\n",
    "        Args:\n",
    "            x (array): List of covariates for a single individual.\n",
    "\n",
    "        Returns:\n",
    "            float: Predicted risk.\n",
    "        \"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "    def return_weights(self):\n",
    "        \"\"\"Plots the weights in a histogram.\"\"\"\n",
    "        \n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "            if type(layer).__name__ == \"Linear\":\n",
    "                weights.append(layer.weight.view(-1))\n",
    "                weights.append(layer.bias)\n",
    "                \n",
    "        plot_weights(torch.cat(weights).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cancer_architecture = [15, 30, 15, 1] \n",
    "cancer_gnn = GNN(cancer_architecture)\n",
    "test_nn(train_df, 'vital_status', 'OS', cancer_gnn, 0.0001, 1000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise weights of the GNN model using a histogram\n",
    "# We expect there to be exponentially more weights in the GNN model than the SNN model\n",
    "cancer_gnn.return_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune the Number of Hidden Layers- NOT RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, modify the loop to iterate over different architectures\n",
    "cancer_nn_losses = []\n",
    "cancer_nn_scores = []\n",
    "cancer_nn_best = [None, 0, 0, 0]\n",
    "\n",
    "# Define a range of hidden layer sizes to test\n",
    "hidden_layer_sizes_range = [\n",
    "    [i] * j  # Creates a list with j elements, each element being i\n",
    "    for i in range(input_size, input_size * 2)\n",
    "    for j in range(1, 3)  # Test models with 1 to 3 hidden layers\n",
    "      # Vary number of neurons per layer\n",
    "]\n",
    "\n",
    "# Loop through the different architectures\n",
    "for layer_sizes in hidden_layer_sizes_range:\n",
    "    # Append 1 for output layer\n",
    "    layer_sizes.append(1)\n",
    "    model = GNN(layer_sizes)\n",
    "    loss, score = test_nn(train_df, 'vital_status', 'OS', model, 0.0001, 1000, False)\n",
    "    loss = loss.detach().numpy()\n",
    "    if score > cancer_nn_best[2]:\n",
    "        cancer_nn_best = [model, loss, score]\n",
    "    cancer_nn_losses.append(loss)\n",
    "    cancer_nn_scores.append(score)\n",
    "\n",
    "# Print the scores of the best architecture\n",
    "print(\"Best GNN: \\\n",
    "    \\n- Architecture: {0} \\\n",
    "    \\n- Loss: {1} \\\n",
    "    \\n- Score: {2}\"\n",
    "    .format(cancer_nn_best[0].layer_sizes, cancer_nn_best[1], cancer_nn_best[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Class Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "class LCA:\n",
    "    \"\"\"Latent Class Analysis model for survival analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, df, event, time, n_components=2, architecture=None):\n",
    "        \"\"\"Initialises the LCA model.\n",
    "\n",
    "        Args:\n",
    "            df (pandas.DataFrame): Dataframe containing the data.\n",
    "            event (str): Name of the event column.\n",
    "            time (str): Name of the time column.\n",
    "            n_components (int, optional): Number of components in the Gaussian Mixture Model. Defaults to 2.\n",
    "            architecture (torch.Tensor, optional): Architecture of the GNN model. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.components = n_components #input size based on number of columns in dataset\n",
    "        self.df = df\n",
    "        self.event = event\n",
    "        self.time = time\n",
    "        \n",
    "        # Initialise the covariates and the sizes\n",
    "        self.input_size = len(self.df.columns) - 3 #subtract OS, vital status and class\n",
    "        \n",
    "        # If the architecture is not None, we use the architecture provided\n",
    "        if architecture is None:\n",
    "            self.architecture = [self.input_size, \n",
    "                                 self.input_size + 1, np.sqrt(self.input_size + 1).astype(int) + 1,\n",
    "                                 1]\n",
    "        else:\n",
    "            self.architecture = architecture\n",
    "        # Initialise the models\n",
    "        # We use GNNs as the models because they are more flexible than SNNs\n",
    "        gnn = GNN(self.architecture)\n",
    "        self.models = [gnn for _ in range(n_components)]\n",
    "        # Initialise the Gaussian Mixture Model, a parametric model which permits clustering\n",
    "        # We set the random seed for reproducibility\n",
    "        self.gmm = GaussianMixture(n_components=n_components, random_state=np.random.seed(42))\n",
    "        \n",
    "        # Stores dataframe for each latent class\n",
    "        self.classes = []\n",
    "        \n",
    "        # Store risks for each each patient\n",
    "        self.risks = []\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"Fits the LCA model to the dataset. and creates the seprate latent classes.\"\"\"\n",
    "        \n",
    "        # Fit Gaussian Mixture Model to the dataset using Expectation Maximisation algorithm\n",
    "        self.gmm.fit(self.df)\n",
    "        \n",
    "        # Assign patients to the latent classes\n",
    "        self.df['LatentClass'] = self.gmm.predict(self.df)\n",
    "    \n",
    "    def train(self, lr=0.00001, epochs=1000, output=False):\n",
    "        \"\"\"Trains the GNN models for each latent class.\n",
    "        \n",
    "        Args:\n",
    "            lr (float, optional): Learning rate. Defaults to 0.00001.\n",
    "            epochs (int, optional): Number of epochs. Defaults to 1000.\n",
    "            output (bool, optional): Whether to print the loss and output loss graph. Defaults to False.\n",
    "        \n",
    "        Returns:\n",
    "            float: Total loss of the model.\n",
    "        \"\"\"  \n",
    "\n",
    "        # Append the dataframes for each latent class to the classes list\n",
    "        for i in range(self.components):\n",
    "            #Latent class column equals the latent class index\n",
    "            self.classes.append(self.df.loc[self.df['LatentClass'] == i].drop('LatentClass', axis=1))\n",
    "            # Remove the latent class column from the dataframe\n",
    "            self.classes[i] = self.df.drop('LatentClass', axis=1)        \n",
    "            \n",
    "        # Initialise the total loss\n",
    "        total_loss = 0\n",
    "        \n",
    "        # For each model, train the model on the data for the given latent class\n",
    "        for i, model in enumerate(self.models):\n",
    "            # Train the model of the latent class\n",
    "            loss = train_model(self.classes[i], self.event, self.time, model, lr, epochs, output)\n",
    "            print(\"Loss for latent class \" + str(i) + \": \" + str(loss.detach().numpy()))\n",
    "            # Add the loss to the total loss (remember the loss is negative)\n",
    "            total_loss += -loss\n",
    "        \n",
    "        return total_loss.detach().numpy()\n",
    "    \n",
    "    def predict(self):\n",
    "        \"\"\"Produces the predictions for each patient in the dataset and computes the final loss and Score.\"\"\"\n",
    "\n",
    "        classes = []\n",
    "        # Iterate over all patients and compute the risk for each patient using the appropriate model\n",
    "        for _, x in self.df.iterrows():\n",
    "            latent_class = x['LatentClass']\n",
    "            # Get the model for the patient\n",
    "            model = self.models[int(latent_class)]\n",
    "            # Remove LatentClass, time and event column from the patient data\n",
    "            x = x.drop('LatentClass').drop(self.event).drop(self.time)\n",
    "            # Make patient a tensor\n",
    "            x = torch.tensor(x.to_numpy(), dtype=torch.float32)\n",
    "            # Compute the risk for the patient\n",
    "            with torch.no_grad():\n",
    "                risk = model.forward(x)\n",
    "            # Append the risk to the list of risks\n",
    "            self.risks.append(risk)\n",
    "            \n",
    "        # Use the predicted risks to compute the score  \n",
    "        return concordance_statistic(self.risks, self.df[self.event].to_numpy(), self.df[self.time].to_numpy(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lca(df, event, time, n=2, architecture=None, lr=0.0001, epochs=1000, output=False):\n",
    "    \"\"\"Function to fit the LCA model and train the GNN models.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Dataframe containing the data\n",
    "        event (str): Name of the event column\n",
    "        time (str): Name of the time column\n",
    "        n (int): Number of latent classes\n",
    "        architecture (list, optional): Architecture of the GNN model. Defaults to None.\n",
    "        lr (float, optional): Learning rate. Defaults to 0.0001.\n",
    "        epochs (int, optional): Number of epochs. Defaults to 1000.\n",
    "        output (bool, optional): Whether to print the loss and output loss graph. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        LCA: Fitted LCA model.\n",
    "    \"\"\"\n",
    "    X = df.copy()\n",
    "\n",
    "    # Initialise LatentClass column as 0\n",
    "    X['LatentClass'] = 0\n",
    "\n",
    "    # Fits the LCA model\n",
    "    model = LCA(X, event, time, n_components=n, architecture=architecture)\n",
    "    model.fit()\n",
    "    \n",
    "    # Print the number of observations in each latent class\n",
    "    print(model.df['LatentClass'].value_counts())\n",
    "    \n",
    "    # Trains the GNN models\n",
    "    loss = model.train(lr, epochs, output)\n",
    "    \n",
    "    score = model.predict()\n",
    "    \n",
    "    print(\"Total Loss: {0} \\nScore: {1}\".format(loss, score))\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Test for implementation\n",
    "cancer_lca1 = lca(train_df, 'vital_status', 'OS', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#While I categorized cancer subtypes into 6 grouping, there are actually 10 different distinct subtypes, so I will choose\n",
    "#this as my number of latent classes. (SCLC, NSCLC melanoma, hepatobiliary, gastric, pancreatic, renal, bladder, esophageal, H and N)\n",
    "\n",
    "cancer_lca10 = lca(train_df, 'vital_status', 'OS', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Include architecture for previously trained GNN model using the number of latent classes \n",
    "#The LCA helps to identify underlying subgroups within the structure of the NN\n",
    "\n",
    "cancer_lca10_arch = lca(train_df, 'vital_status', 'OS', 10, cancer_architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a simple CNN\n",
    "class CNN(torch.nn.Module):\n",
    "    \"\"\"Simple CNN model for survival analysis.\"\"\"\n",
    "    def __init__(self, df, event, time, width=0, height=0):\n",
    "        \"\"\"Initialises the CNN model.\n",
    "        \n",
    "        Args:\n",
    "            df (pandas.DataFrame): Dataframe containing the data\n",
    "            event (str): Name of the event column\n",
    "            time (str): Name of the time column\n",
    "            width (int, optional): Width of the input image. Defaults to 0.\n",
    "            height (int, optional): Height of the input image. Defaults to 0.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Dataframe containing the data\n",
    "        self.df = df\n",
    "        \n",
    "        # Event and time columns\n",
    "        self.event = event\n",
    "        self.time = time\n",
    "                \n",
    "        # Dataframe containing covariates\n",
    "        self.dfx = self.df.drop([self.event, self.time], axis=1, inplace=False)\n",
    "        # Convert dataframe to numpy array\n",
    "        self.df_samples = self.dfx.to_numpy()\n",
    "        \n",
    "        # Calculate the optimal width and height based on the number of covariates\n",
    "        #in the dataset unless otherwise specified.\n",
    "        if width == 0 or height == 0:\n",
    "            self.width, self.height = self.get_optimal_dimensions(self.dfx.shape[1])\n",
    "        else:\n",
    "            self.width = width\n",
    "            self.height = height\n",
    "        \n",
    "        # Use random seed for reproducibility\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        # Create the model\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        #input channel = 1. This generally refers to greyscale data (eg 3 for RGB image)\n",
    "        #output channel = 32. I left as is, based on what the authors did\n",
    "        #kernel_size = 3*3, determines receptive field. Other choices include 5*5, 7*7\n",
    "        #stride = the step size.\n",
    "        #padding = # of pixels added to borders of input data\n",
    "        \n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        #This refers to the max-pooling layer- the kernel is the size of the pooling layer, the stride\n",
    "        #is the step size, the padding as above.\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(32 * (self.width // 2) * (self.height // 2), 128)\n",
    "        #defines the fully connected linear layer\n",
    "\n",
    "        self.fc2 = torch.nn.Linear(128, 1)\n",
    "        self.layers = [self.conv1, self.pool, self.fc1, self.fc2]\n",
    "        self.model = torch.nn.Sequential(*self.layers)\n",
    "        \n",
    "        # Transform the data\n",
    "        self.transform()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Produces risk scores for the given patient image.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Patient image\n",
    "\n",
    "        Returns:\n",
    "            float: Risk score\n",
    "        \"\"\"\n",
    "        x = self.pool(torch.nn.functional.relu(self.conv1(x)))\n",
    "        #apply first convolutional layer to input tensor --> ReLU activation then max pooling.\n",
    "\n",
    "        x = x.view(-1, 32 * (self.width // 2) * (self.height // 2))\n",
    "        #reshapes into a 1D tensor\n",
    "\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        #apply 1st fully connected layer to reshaped tensor\n",
    "        x = self.fc2(x)\n",
    "        #apply 2nd fully connected layer to output of previous layer --> final output tensor\n",
    "        return x\n",
    "    \n",
    "    def transform(self):\n",
    "        \"\"\"Transforms the data to be used by the model.\"\"\"\n",
    "        \n",
    "        # Convert features to images\n",
    "        self.images = [self.features_to_image(x, self.width, self.height) for x in self.df_samples]\n",
    "        # Prepare the data for the CNN\n",
    "        self.tensors = [torch.tensor(image, dtype=torch.float32).unsqueeze(0) for image in self.images]\n",
    "        self.tensor = torch.stack(self.tensors)\n",
    "        self.labels = torch.tensor(self.df[self.event].to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    def train(self, lr=0.0001, epochs=1000, output=True):\n",
    "        \"\"\"Trains the model.\n",
    "        \n",
    "        Args:\n",
    "            lr (float): Learning rate\n",
    "            epochs (int): Number of epochs\n",
    "            output (bool, optional): Whether to output the loss plot. Defaults to True.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Note we use a custom optimizer here as the original classes are not suitable\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        scores = []\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = self.forward(self.tensor)\n",
    "\n",
    "            # Calculate concordance statistic\n",
    "            score = concordance_statistic(outputs, self.df[self.event].to_numpy(), self.df[self.time].to_numpy(), output=False)\n",
    "\n",
    "            # Calculate the negative of the concordance statistic to maximize it\n",
    "            loss = -torch.tensor(score, requires_grad=True)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Save score\n",
    "            scores.append(score)\n",
    "            \n",
    "        # Plot losses\n",
    "        if output:\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.plot(scores)\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Score')\n",
    "            plt.show()\n",
    "            \n",
    "    def get_optimal_dimensions(self, n_features):\n",
    "        \"\"\"Calculate the optimal width and height for a given number of features.\n",
    "        \n",
    "        Args:\n",
    "            n_features (int): Number of features\n",
    "            \n",
    "        Returns:\n",
    "            tuple: Optimal width and height\n",
    "        \"\"\"\n",
    "        \n",
    "        #calculate square root of features\n",
    "        sqrt_n_features = int(np.sqrt(n_features))\n",
    "        if sqrt_n_features * sqrt_n_features == n_features:\n",
    "            return sqrt_n_features, sqrt_n_features\n",
    "        else:\n",
    "            for i in range(sqrt_n_features + 1, n_features + 1):\n",
    "                if n_features % i == 0:\n",
    "                    return i, n_features // i\n",
    "        \n",
    "    # Convert features to images\n",
    "    def features_to_image(self, features, width, height):\n",
    "        \"\"\"Converts features given a patient to an image.\n",
    "        \n",
    "        Args:\n",
    "            features (np.array): Features of a patient\n",
    "            width (int): Width of the image\n",
    "            height (int): Height of the image\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Image of the features\n",
    "        \"\"\"\n",
    "        \n",
    "        # Normalize the features\n",
    "        normalized_features = (features - np.min(features)) / (np.max(features) - np.min(features))\n",
    "\n",
    "        # Calculate the required number of padding zeros\n",
    "        num_features = len(features)\n",
    "        target_size = width * height\n",
    "        padding_zeros = target_size - num_features\n",
    "\n",
    "        # If padding is required, add zeros to the features\n",
    "        if padding_zeros > 0:\n",
    "            normalized_features = np.concatenate((normalized_features, np.zeros(padding_zeros)))\n",
    "\n",
    "        # Reshape the features to the desired width and height\n",
    "        image = normalized_features.reshape(width, height)\n",
    "        return image\n",
    "        \n",
    "    def return_weights(self):\n",
    "        \"\"\"Plots the weights in a histogram.\"\"\"\n",
    "        \n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'weight'):\n",
    "                weights.append(layer.weight.view(-1))\n",
    "                \n",
    "        plot_weights(torch.cat(weights).detach().numpy())\n",
    "        \n",
    "    def plot_image(self, x):\n",
    "        \"\"\"Plots the feature image of a patient. Converts the image to an actual image.\n",
    "\n",
    "        Args:\n",
    "            x (int): Index of the patient in the dataframe.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.imshow(self.images[x], cmap='gray')\n",
    "        plt.show()\n",
    "        \n",
    "    def score(self):\n",
    "        return f\"Score: {concordance_statistic(self(self.tensor).detach().numpy(), self.df[self.event].to_numpy(), self.df[self.time].to_numpy(), True)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm not sure yet what this does\n",
    "\n",
    "cancer_cnn = CNN(cancer_scaled, 'vital_status', 'OS')\n",
    "cancer_cnn.plot_image(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_cnn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_cnn.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_cnn.return_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
