{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lecture 5\n",
    "\n",
    "#Git reflog can be used to review all recent commits\n",
    "\n",
    "#Cross validation- in extremely massive datasets, may not want to use 20% of data in CV\n",
    "#May not do hyperparameter tuning in logistic regression (so can leave empty)\n",
    "\n",
    "#K fold can also be used to split the dataset. In K fold, shuffling is off by default\n",
    "#Look into cross_val_score\n",
    "\n",
    "#Scaling of data- standardization and normalization\n",
    "#In logistic regression, if data is not scaled, then it may not converge\n",
    "\n",
    "#Model may run faster if you reduced the pixels (right now 28*28)\n",
    "\n",
    "#Grid search can take a very long time to do. \n",
    "\n",
    "#Self organizing maps reduce the data down into a plane\n",
    "\n",
    "#Learning rate- how large of a step do I need to take in my learning rate. \n",
    "\n",
    "#Keras is a good package for neural networks. \n",
    "\n",
    "#Flatten layer to convert 2D data into 1D array\n",
    "#If have 10 digits, you have 10 layers\n",
    "#Dropout layer is selectively dropping data to prevent overfitting?\n",
    "#Can modify how many neurons we have in our dense middle layer (in the notes example, we have 128 neurons, which is modifyable.\n",
    "#Deciding on an activation function- we will discuss later. \n",
    "#Softmax is used for output layer\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
